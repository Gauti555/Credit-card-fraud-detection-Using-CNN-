# -*- coding: utf-8 -*-
"""CNN for credit card fraud detection .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V8-9M51Xp42S-RLA1qkDCwzqkih8N_va

# step 1 : installation and set up
"""

pip install  tensorflow-gpu

import tensorflow as tf 
print (tf.__version__)

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt

"""# step 2 : importing the dataset from kaggle to goggle colab"""

# insatll kaggale API
! pip install -q kaggle

# create a directory as kaggle 
! mkdir -p ~/.kaggle

# importing kaggle API key to google colab 
from google.colab import files 
uploaded = files.upload()

# copy API key to kaggle directory
! cp kaggle.json ~/.kaggle

# dissabele the API key 
! chmod 600 /root/.kaggle/kaggle.json

# list of dataset 
! kaggle datasets list

# import the dataset 
! kaggle datasets download -d mlg-ulb/creditcardfraud

# unziping the dataset
! unzip /content/creditcardfraud.zip

dataset_1 = pd.read_csv ('/content/creditcard.csv')

dataset_1.head()

"""# step 3 : data preprocessing"""

dataset_1.shape

# checking the null values 
dataset_1.isnull().sum()

dataset_1.info()

# observation in each class 
dataset_1['Class'].value_counts()

# balanced the data set
fraud = dataset_1[dataset_1['Class']==1]
non_fraud = dataset_1[dataset_1['Class']==0]

fraud.shape,non_fraud.shape

# random selection of sample
non_fraud_t = non_fraud.sample(n=492)

non_fraud_t.shape

# merge the dataset 
dataset = fraud.append(non_fraud_t,ignore_index=True)

print(dataset)

# observation in each class 
dataset['Class'].value_counts()

# matrix of features 
x=dataset.drop(labels=['Class'],axis=1)

# dependent variable 
y=dataset ['Class']

x.shape,y.shape

# spliting the dataset into train and test 
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=0)

x_train.shape,x_test.shape

# feature scalling 
from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
x_train=sc.fit_transform(x_train)
x_test =sc.transform(x_test)

x_train

y_train =y_train.to_numpy()

y_test = y_test.to_numpy()

x_train.shape , x_test.shape

#reshape the dataset 
x_train = x_train.reshape(787,30,1)

x_test = x_test.reshape(197,30,1)



"""# step 3 : build the model"""

# define the object 
model= tf.keras.models.Sequential()
# sequence of layer

# adding first cnn layer
# 1) filters (kernels) =64
# 2) kernal size = 3
# 3) padding = same 
# 4) activation function = relu
# 5) input shape  = (32,32,3)
model.add(tf.keras.layers.Conv1D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=(30,1))) 

# batchnormalization 
model.add(tf.keras.layers.BatchNormalization())

# maxpool parameter
# 1) pool size =2
# 2) strides =2
# 3) padding = valid
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# adding the dropout layer it is regularisation technique where randomly selected newrons eleminated during training process 
model.add(tf.keras.layers.Dropout(0.2))

# adding second  cnn layer
# 1) filters (kernels) =64
# 2) kernal size = 3
# 3) padding = same 
# 4) activation function = relu
# 5) input shape  = (32,32,3)
model.add(tf.keras.layers.Conv1D(filters=64,kernel_size=3,padding='same',activation='relu',)) 
#But the problem appears in the intermediate layers because the distribution of the activations is constantly changing during training.
# This slows down the training process because each layer must learn to adapt themselves to a new distribution in every training step. 
#This problem is known as internal covariate shift.
# batchnormalization 
model.add(tf.keras.layers.BatchNormalization())

# maxpool parameter
# 1) pool size =2
# 2) strides =2
# 3) padding = valid
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# adding the dropout layer it is regularisation technique where randomly selected newrons eleminated during training process 
model.add(tf.keras.layers.Dropout(0.3))

# adding the flatteniung layer 
# by adding the falttening layer we are converting the array into the vector 
model.add(tf.keras.layers.Flatten())

# adding the first dense layerA
model.add(tf.keras.layers.Dense(units=64,activation='relu'))

# addind dropout 
model.add(tf.keras.layers.Dropout(0.3))

# adding the second dense layers 
model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))

model.summary()

opt = tf.keras.optimizers.Adam(learning_rate=0.0001)

# compile the model 

model.compile(loss='binary_crossentropy', optimizer=opt ,metrics=['accuracy'])

"""# step 5 : train the model"""

history = model.fit(x_train,y_train, epochs=25, validation_data=(x_test,y_test))

# model prediction 
y_pred = model.predict_classes(x_test)

print (y_pred[5]), print (y_test[5])

# confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test,y_pred)
print (cm)

acc_cm =accuracy_score(y_test,y_pred)
print (acc_cm)

"""# step 6 : learning curve"""

def learning_curve (history,epoch):
  # training vs validation accuracy
  epoch_range = range (1,epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train','val'], loc ='upper left')
  plt.show()


  #training vs validation loss
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('Epoch')
  plt.legend(['Train','val'], loc ='upper left')
  plt.show()

learning_curve (history,25)